[
  {
    "objectID": "posts/xavier-initialization.html",
    "href": "posts/xavier-initialization.html",
    "title": "Why Neural Networks Forget",
    "section": "",
    "text": "Vanishing gradients\nNeural networks adjust their parameters through the well-known backpropagation algorithm. The cost function gradients are propagated from the output layer back to all layers of the model, enabling parameter updates. The network‚Äôs training depends on gradients being properly propagated. If the gradients fail to reach the earlier layers, the network won‚Äôt train as expected. Can something like this actually happen? Let‚Äôs see a practical example.\nConsider a layer with a sigmoid activation function. From basic calculus, we know that \\(\\large \\frac{d\\sigma(x)}{dx}=\\sigma(x)(1-\\sigma(x))\\) reaches its maximum value of 0.25 at \\(x=0\\), and is zero when \\(\\sigma(x)\\) saturates at 0 or 1. For more clarity, the following plot shows both the sigmoid function and its derivative.\n\n\n\nTo update the weights of this layer, we need to compute \\(\\large \\frac{\\partial L}{\\partial W^{(L)}}\\), which by the chain rule is: \\[\n\\large \\frac{\\partial L}{\\partial W^{(L)}}=\\frac{\\partial z^{(L)}}{\\partial W^{(L)}}\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\frac{\\partial L}{\\partial a^{(L)}}\n\\] Focusing on \\(\\large \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\): \\[\n\\large \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}=\\sigma(z^{(L)})(1-\\sigma(z^{L}))\n\\] If \\(\\sigma(z^{(L)})=1\\) or \\(\\sigma(z^{(L)})=0\\), then the gradient is zero. This phenomenon is called gradient vanishing. If there‚Äôs no gradient, there‚Äôs no learning. Since the derivative of the sigmoid is always in the range \\([0,0.25]\\), it‚Äôs prone to vanishing gradients.\n\n\nThe risks of random initialization\nOne commonly overlooked detail when training a neural network is weight initialization. If the initial weights push the activation values into saturation zones (0 or 1), gradient vanishing can occur. For example, if weights are initialized as \\(W\\sim\\mathcal{N}(0,1)\\), some values might lie far outside the optimal region. A natural fix might be to initialize weights as \\(W\\sim\\mathcal{U}[-1,1]\\), keeping values within range. This may work for small, shallow networks. Let \\(n\\) be the number of neurons in layer \\(l-1\\), meaning the next layer has \\(n\\) inputs. The pre-activation of a neuron is \\(\\sum_{i=1}^n w_{i}x_{i}\\), If \\(n\\) is large, the sum may become too large and lead to saturation. So, uniform initialization isn‚Äôt ideal either.\n\n\nThe Xavier Initialization solution\nIf initializing with zeros, Gaussian, or uniform distributions is problematic, what‚Äôs the solution? Fortunately, Xavier Glorot and Yoshua Bengio proposed one in 2010. In their paper Understanding the Difficulty of Training Deep Feedforward Neural Networks, they introduced what we now call Xavier Initialization.\n\nNotation\nLet \\(s_{i}=z_{i}W_{i}+b_{i}\\) be the pre-activation, so \\(z_{i+1}=f(s_{i})\\). The analysis focuses near the origin, where the sigmoid behaves approximately linearly \\(\\sigma(x)\\approx x\\) and \\(\\sigma'(x)\\approx 1\\). The goal is to keep the variance of \\(z_{i+1}\\) (forward pass) and the variance of gradients (backward pass) of the same order in every layer.\n\n\nForward Pass\nWe write \\(s_{i}=\\sum_{j=1}^n z_{j}^{i} w_{i}^{j}\\), assuming all \\(w_{i}^{j}\\) are independent with zero mean and variance \\(\\text{Var}[w_{i}]\\), and similarly for \\(w_{j}^{i}\\). Then: \\[\n\\large \\text{Var}[s_{i}]=\\sum_{j=1}^{n_{i}}\\text{Var}[z_{i}^{j}]¬∑\\text{Var}[w_{i}^{j}]=n_{i}¬∑\\text{Var}[z_{i}]\\text{Var}[w_{i}]\n\\] Near the origin, \\(\\large \\text{Var}[z_{i+1}]\\approx \\text{Var}[S_{i}]\\), so we want: \\[\n\\large \\mathrm{Var}[z_{i+1}] \\approx \\mathrm{Var}[z_i] \\Rightarrow n_i \\cdot \\mathrm{Var}[z_i] \\cdot \\mathrm{Var}[w_i] \\approx \\mathrm{Var}[z_i] \\Rightarrow n_i \\cdot \\mathrm{Var}[w_i] \\approx 1\n\\]\n\n\nBackward Pass\nTo compute the gradient \\(\\large \\delta_{i}=\\frac{\\partial L}{\\partial s_{i}}\\), we use the chain rule and the linear approximation \\(\\large \\delta_{i} \\approx W_{i+1}^{T}\\delta_{i+1}\\). Therefore: \\[\n\\mathrm{Var}[\\delta_i] \\approx n_{i+1} \\cdot \\mathrm{Var}[W_i] \\cdot \\mathrm{Var}[\\delta_{i+1}] \\Rightarrow n_{i+1} \\cdot \\mathrm{Var}[W_i] \\approx 1\n\\]\n\n\nA balanced solution\nWe now have two desired conditions:\n\n\\(n_i \\cdot \\mathrm{Var}[w_i] \\approx 1\\)\n\\(n_{i+1} \\cdot \\mathrm{Var}[W_i] \\approx 1\\)\n\nIf \\(n_{i}=n_{i+1}\\), both are satisfied. When not, Glorot and Bengio propose a compromise: \\[\n\\large \\text{Var}[W_{i}]=\\frac{2}{n_{i}+n_{i+1}}\n\\] To implement this in practice, with a uniform distribution \\(W_{i}\\sim \\mathcal{U}[-a,a]\\), note that: \\[\n\\large \\frac{2}{n_i + n_{i+1}} = \\frac{a^2}{3} \\Rightarrow a = \\sqrt{\\frac{6}{n_i + n_{i+1}}}\n\\] This yields the final Xavier Initialization: \\[\n\\Large W_{i}=\\mathcal{U}[-\\sqrt{\\frac{6}{n_{i}+n_{i+1}}},+\\sqrt{\\frac{6}{n_{i}+n_{i+1}}}]\n\\]\n\n\n\nReferences\n\nXavier Glorot, Yoshua Bengio, Understanding the Difficulty of Training Deep Feedforward Neural Networks, 2010."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Sigmoid Tales",
    "section": "",
    "text": "Welcome to the Sigmoid Tales ‚Äî a curated collection of technical yet accessible essays on Machine Learning & Deep Learning. Whether you‚Äôre just starting or brushing up on foundational concepts, this repository aims to clarify complex ideas through clear writing, visual intuition and well-grounded mathematics."
  },
  {
    "objectID": "index.html#overview-of-the-project",
    "href": "index.html#overview-of-the-project",
    "title": "The Sigmoid Tales",
    "section": "Overview of the project",
    "text": "Overview of the project\nThe Sigmoid Tales is a series of short essays on key ML & DL topics. the main goal is to demystify the core concepts and techniques of the field, explaining not only the how, but also the why. This collection also aims to bridge the gap between hard mathematics, complex theoretical concepts and practical applications. Each ‚Äútale‚Äù focuses on a specific concept, covering most of the areas in Machine Learning. They are labeled by colors, each one indicating the level needed to understand the content:\n\nüü¢ Beginner: Usually intuitions and very basic concepts.\nüü° Medium: A bit more advanced, but still basic concepts.\nüü† Medium/Advanced: You‚Äôll need to completely master the beginner level tales.\nüî¥ Advanced: Math-heavy, State-of-the-art topics.\n\nFeel free to read, share and learn from this collection. Just don‚Äôt feed it directly to Skynet."
  },
  {
    "objectID": "index.html#recent-articles",
    "href": "index.html#recent-articles",
    "title": "The Sigmoid Tales",
    "section": "üìö Recent articles",
    "text": "üìö Recent articles"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "drafts/optimizers.html",
    "href": "drafts/optimizers.html",
    "title": "How do Neural Networks learn?",
    "section": "",
    "text": "How do Neural Networks learn?\nTraining a neural network has never been easier. You define the architecture with Keras (or PyTorch, if you know what‚Äôs good), set the optimizer and run the model.train command. Many times, most people don‚Äôt pay attention to the optimizer. The would use Adam and start the training. However, as you get involved with architectures a bit more complex than the classical feed-forward neural network for the MNIST digits dataset, you may come across quite a frustrating situation: after a few epochs, the model won‚Äôt improve the loss metric. And maybe (just maybe) the reason could be the optimizer you are using.\n\nGradient descent: how to learn from the mistakes\nSo, how exactly does a neural network learn from its mistakes? The answer lies in one of the simplest yet powerful ideas in optimization: gradient descent.\nThe gradient of the loss with respect to a certain parameters tells us the direction in which te loss increases the most. So, to reduce the loss, we should take a small step in the opposite direction. Mathematically, we usually represent the model‚Äôs parameters with the Greek letter \\(\\theta\\), and \\(\\mathcal{L}(\\theta)\\) is the loss function. Hence, the update rule is: \\[\n\\large \\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_{\\theta} \\mathcal{L}(\\theta)\n\\] Here, \\(\\eta\\) is the learning rate, a small constant which controls the step size. Repeat this process over and over, and the model will gradually adjust its parameters to reduce the error, ideally finding a minimum of the loss function. Here is a visual representation of this process:\n\n\n\ngradient_descent\n\n\n\n\n\nHands-on exercise: training a neural network\nEven though it is important to have a solid grasp of the theory, one must be able to quickly detect the problems when working in a real scenario. To connect theory with practice, we are going to run the most typical experiment: MNIST digit recognizer."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Why Neural Networks Forget\n\n\nThe Vanishing Gradient Problem Explained\n\n\n\n\n\n\n\n\nNo matching items"
  }
]