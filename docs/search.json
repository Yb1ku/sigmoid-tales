[
  {
    "objectID": "posts/optimizers.html",
    "href": "posts/optimizers.html",
    "title": "How Do Neural Networks Learn?",
    "section": "",
    "text": "How do Neural Networks learn?\nTraining a neural network has never been easier. You define the architecture with Keras (or PyTorch, if you know what‚Äôs good), set the optimizer and run the model.train command. Many times, most people (myself included) don‚Äôt pay attention to the optimizer. They simply choose Adam and start the training without questioning if it‚Äôs the right choice . Despite Adam is the default choice for many people, it is good to know why it is considered one of the best optimizers out there. To fully understand Adam, one needs to start with the basics: gradient descent.\n\nGradient descent: how to learn from mistakes\nSo, how exactly does a neural network learn from its mistakes? The answer lies in one of the simplest yet powerful ideas in optimization: gradient descent.\nThe gradient of the loss with respect to certain parameters tells us the direction in which the loss increases the most. So, to reduce the loss, we should take a small step in the opposite direction. Mathematically, we usually represent the model‚Äôs parameters with the Greek letter \\(\\theta\\), and \\(\\mathcal{L}(\\theta)\\) is the loss function. Hence, the update rule is: \\[\n\\large \\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_{\\theta} \\mathcal{L}(\\theta)\n\\] Here, \\(\\eta\\) is the learning rate, a small constant which controls the step size. Repeat this process over and over, and the model will gradually adjust its parameters to reduce the error, ideally finding a minimum of the loss function. Here is a visual representation of this process:\n\n\n\ngradient_descent\n\n\nWe usually work with really big datasets, so using all the examples is often not feasible. To solve this problem, the are a few modifications we can make to gradient descent:\n\nStochastic Gradient Descent: Instead of using all the examples in the dataset for each step, we only use one.\n\n‚úîÔ∏è It is much faster than traditional gradient descent.\n‚ùå It is less precise, which can hinder convergence.\n\nMini-batch Gradient Descent: Computes the gradient using a small subset (batch) of the dataset.\n\n‚úîÔ∏è Faster than traditional gradient descent.\n‚úîÔ∏è Balances precision and speed.\n‚ùå Less precise than traditional gradient descent.\n\n\n\n\n\n\nLearning is a two-phase process\nThe most critical hyperparameters in optimization is the learning rate. Gradient Descent and its early variants typically use a constant value for this parameter, which might seem sufficient at first glance. However, training a model can often be understood as a two-phase process: an initial phase of rapid adaptation, followed by a slower stage of fine-tuning.\nIn the early phase, the model makes several large parameter updates, as it quickly learns the general structure of the data - some sort of ‚Äúheavy learning process‚Äù. Once this phase ends, the model enters a refinement stage, where more subtle adjustments are required to further improve performance. These smaller updates would require a lower learning rate, in order to avoid overshooting the minimum. We can see an example of this below:\n\n\n\novershooting\n\n\nFor this particular loss function, setting the learning rate to 1.01 would make the optimizer to overshoot, preventing convergence to the minimum. On the other hand, setting the learning rate to less than 1 would allow the optimizer to eventually find the minimum.\nWhile mastering the art of selecting the right learning rate is essential, a constant learning rate can sometimes be limiting, as it lacks the flexibility to adapt to the evolving needs of the optimization process. Hence, a natural step is to design new optimizers which dynamically adjust the learning rate, either over time or based on the gradients‚Äô behavior.\n\nAdaGrad: Adaptive Gradient\nAdaGrad, short for Adaptive Gradient, was the first step towards dynamic learning rate adjustment. Instead of using a fixed learning rate, it reduces it based on the cumulative gradient of each parameter. Let‚Äôs dive into the math:\nSuppose we use \\(\\theta \\in \\mathbb{R}^n\\) to denote the vector containing the \\(n\\) parameters of the model. We will use \\(g_{t}=\\nabla_{\\theta}\\mathcal{L}(\\theta_{t})\\) to refer to the gradient of the parameters at epoch \\(t\\). For a specific parameter \\(\\theta_{i}\\), we can compute the cumulative sum of its gradients as: \\[\n\\large G_{t,i} = \\sum_{\\tau=1}^{t} g_{\\tau,i}^2=G_{t-1,i}+g_{t,i}^{2}\n\\] Think of \\(G_{t,i}\\) as the cumulative effort the optimizer has made during the training to update parameter \\(\\theta_{i}\\). We now reformulate the update rule as follows: \\[\n\\large \\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,i}} + \\epsilon} \\cdot g_{t,i}\n\\] If the parameter has received large or frequent updates (i.e., \\(G_{t,i}\\) is large), the AdaGrad assumes it has already undergone substantial learning. As a result, its learning rate is reduced, allowing for more precise adjustments. This is how AdaGrad dynamically adjusts the learning rate per parameter.\n\n\nRMSProp\nEven though lowering the learning rate over time helps with the overshooting problem, AdaGrad may reduce it too quickly. In practice, if the model spends several epochs in the early stages of training, the cumulative gradients can grow large enough to shrink the effective learning rate dramatically. As a result, the model may stop learning long before reaching a satisfactory solution.\nTo avoid this. Geoffrey Hinton proposed RMSProp, a simple yet effective modification to the AdaGrad algorithm.\nInstead of relying on the entire history of gradients, RMSProp replaces \\(G_{t,i}\\) with an exponential moving average (EMA) of the squared gradients. RMSProp replaces the cumulative sum with: \\[\n\\large E[g^2]_{t,i}=\\gamma ¬∑ E[g^2]_{t-1,i}+(1-\\gamma)¬∑g^2_{t,i}\n\\] Here, \\(\\gamma \\in[0,1)\\) is the decay factor, which determines how much weight is given to past gradients. For example, if \\(\\gamma=0.8\\), then \\(E[g^2]_{t,i}\\) will be formed 80% by the old gradients and 20% by the new one. This way, we can control the degree to which past gradient influence the parameter update. The update rule becomes: \\[\n\\large \\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{E[g^2]_{t,i}} + \\epsilon} \\cdot g_{t,i}\n\\] This formulation ensures that parameters associated with frequently large gradients receive smaller updates, while still allowing other parameters to keep learning. Unlike AdaGrad, RMSProp avoids vanishing learning rates by gradually forgetting older information.\n\n\nAdam\nRMSProp solved the learning rate shrinking problem by using an exponentially decaying average of squared gradients. However, there is a subtle issue it fails to address. Imagine we have a ‚Äúnoisy‚Äù gradient in a non-convex zone such as: \\[\n\\large g_t \\in \\{-0.9,\\ +1.0,\\ -0.8,\\ +1.1,\\ \\dots\\}\n\\] RMSProp would only look at the magnitude of the gradients, so \\(G_t\\) would be: \\[\n\\large G_t \\in \\{+0.81,\\ +1.81,\\ +2.45,\\ +3.66,\\ \\dots\\}\n\\] According to the logic of RMSProp, such a history suggests that significant updates have already been made, so it lowers the effective learning rate accordingly. However, the fact that the magnitude of the gradients are big and the signs are not consistent could be an indicator that we are not close to a minimum, but rather in a region with bumps or high-frequency noise in the loss surface. This is often the case when using mini-batches instead of the full dataset. RMSProp only uses the magnitude of the gradients, it has no way to detect this directional inconsistency.\nThis is where Adam steps in. It incorporates a running average for both the squared gradients (like RMSProp) and the gradients themselves. This way, Adam can simultaneously adapt the learning rate and track the overall direction of the descent, enabling it to perform better in noisy regions.\nWe define 4 new terms:\n\n\\(m_t\\): exponential moving average of \\(g_t\\)\n\\(v_t\\): exponential moving average of \\(g^2_t\\)\n\\(\\beta_1, \\beta_2\\): decay factors\n\nBoth \\(m_t\\) and \\(v_t\\) are updated as follows: \\[\n\\large m_t=\\beta_1 ¬∑m_{t-1} + (1-\\beta_1)¬∑g_t\n\\] \\[\n\\large v_t=\\beta_2 ¬∑v_{t-1} + (1-\\beta_2)¬∑g^2t\n\\]\nSince \\(m_t\\) and \\(v_t\\) are initialized to zero, we define: \\[\n\\large \\hat{m}_t=\\frac{m_t}{1-\\beta_1}\n\\] \\[\n\\large \\hat{v}_t=\\frac{v_t}{1-\\beta_2}\n\\]\nWe are now ready to compute the update rule for Adam: \\[\n\\large \\theta_{t+1}=\\theta_t +\\eta¬∑\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}\n\\]\nThis balance of directionality and adaptive learning rate makes Adam a robust choice for many modern deep learning tasks.\n\n\n\nComparing the optimizers\nEven though it is important to have a solid grasp of the theory, one must be able to quickly detect the problems when working in a real scenario. To connect theory with practice, we are going to run a simple example on a loss function.\n\n\n\nanimation\n\n\nThe table below summarizes the main pros and cons of each optimizer:\n\n\n\n\n\n\n\n\nOptimizer\nPros\nCons\n\n\n\n\nAdaGrad\nAdapts \\(\\eta\\) for each parameter. Useful for sparse features.\n\\(\\eta\\) can be reduced too quickly.\n\n\nRMSProp\nMore stable \\(\\eta\\) update than AdaGrad.\nDoesn‚Äôt take gradient‚Äôs direction into account.\n\n\nAdam\nCombines advantages of RMSProp and momentum.\nMay lead to overfitting. Sensitive to \\(\\eta\\) and default hyperparameters.\n\n\n\nIn practice, no optimizer is universally best. Understanding their strengths and weaknesses is key to unlocking the model‚Äôs full potential. After reading this post, at least you will know why Adam is the default choice for everyone!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Sigmoid Tales",
    "section": "",
    "text": "Welcome to the Sigmoid Tales ‚Äî a curated collection of technical yet accessible essays on Machine Learning & Deep Learning. Whether you‚Äôre just starting or brushing up on foundational concepts, this repository aims to clarify complex ideas through clear writing, visual intuition and well-grounded mathematics."
  },
  {
    "objectID": "index.html#overview-of-the-project",
    "href": "index.html#overview-of-the-project",
    "title": "The Sigmoid Tales",
    "section": "Overview of the project",
    "text": "Overview of the project\nThe Sigmoid Tales is a series of short essays on key ML & DL topics. the main goal is to demystify the core concepts and techniques of the field, explaining not only the how, but also the why. This collection also aims to bridge the gap between hard mathematics, complex theoretical concepts and practical applications. Each ‚Äútale‚Äù focuses on a specific concept, covering most of the areas in Machine Learning. They are labeled by colors, each one indicating the level needed to understand the content:\n\nüü¢ Beginner: Usually intuitions and very basic concepts.\nüü° Medium: A bit more advanced, but still basic concepts.\nüü† Medium/Advanced: You‚Äôll need to completely master the beginner level tales.\nüî¥ Advanced: Math-heavy, State-of-the-art topics.\n\nFeel free to read, share and learn from this collection. Just don‚Äôt feed it directly to Skynet."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "How Do Neural Networks Learn?\n\n\nAn intuitive introduction to optimization algorithms like AdaGrad, RMSProp and Adam.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Neural Networks Forget\n\n\nThe Vanishing Gradient Problem Explained\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/xavier-initialization.html",
    "href": "posts/xavier-initialization.html",
    "title": "Why Neural Networks Forget",
    "section": "",
    "text": "Why Neural Networks Forget: The Vanishing Gradient Problem Explained\nNeural networks adjust their parameters through the well-known backpropagation algorithm. The cost function gradients are propagated from the output layer back to all layers of the model, enabling parameter updates. The network‚Äôs training depends on gradients being properly propagated. If the gradients fail to reach the earlier layers, the network won‚Äôt train as expected. Can something like this actually happen? Let‚Äôs see a practical example.\nConsider a layer with a sigmoid activation function. From basic calculus, we know that \\(\\large \\frac{d\\sigma(x)}{dx}=\\sigma(x)(1-\\sigma(x))\\) reaches its maximum value of 0.25 at \\(x=0\\), and is zero when \\(\\sigma(x)\\) saturates at 0 or 1. To update the weights of this layer, we need to compute \\(\\large \\frac{\\partial L}{\\partial W^{(L)}}\\), which by the chain rule is: \\[\n\\large \\frac{\\partial L}{\\partial W^{(L)}}=\\frac{\\partial z^{(L)}}{\\partial W^{(L)}}\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\frac{\\partial L}{\\partial a^{(L)}}\n\\] Focusing on \\(\\large \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\): \\[\n\\large \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}=\\sigma(z^{(L)})(1-\\sigma(z^{L}))\n\\] If \\(\\sigma(z^{(L)})=1\\) or \\(\\sigma(z^{(L)})=0\\), then the gradient is zero. This phenomenon is called gradient vanishing. If there‚Äôs no gradient, there‚Äôs no learning. Since the derivative of the sigmoid is always in the range \\([0,0.25]\\), it‚Äôs prone to vanishing gradients.\n\n\nThe risks of random initialization\nOne commonly overlooked detail when training a neural network is weight initialization. If the initial weights push the activation values into saturation zones (0 or 1), gradient vanishing can occur. For example, if weights are initialized as \\(W\\sim\\mathcal{N}(0,1)\\), some values might lie far outside the optimal region. A natural fix might be to initialize weights as \\(W\\sim\\mathcal{U}[-1,1]\\), keeping values within range. This may work for small, shallow networks. Let \\(n\\) be the number of neurons in layer \\(l-1\\), meaning the next layer has \\(n\\) inputs. The pre-activation of a neuron is \\(\\sum_{i=1}^n w_{i}x_{i}\\), If \\(n\\) is large, the sum may become too large and lead to saturation. So, uniform initialization isn‚Äôt ideal either.\n\n\nThe Xavier Initialization solution\nIf initializing with zeros, Gaussian, or uniform distributions is problematic, what‚Äôs the solution? Fortunately, Xavier Glorot and Yoshua Bengio proposed one in 2010. In their paper Understanding the Difficulty of Training Deep Feedforward Neural Networks, they introduced what we now call Xavier Initialization.\n\nNotation\nLet \\(s_{i}=z_{i}W_{i}+b_{i}\\) be the pre-activation, so \\(z_{i+1}=f(s_{i})\\). The analysis focuses near the origin, where the sigmoid behaves approximately linearly \\(\\sigma(x)\\approx x\\) and \\(\\sigma'(x)\\approx 1\\). The goal is to keep the variance of \\(z_{i+1}\\) (forward pass) and the variance of gradients (backward pass) of the same order in every layer.\n\n\nForward Pass\nWe write \\(s_{i}=\\sum_{j=1}^n z_{j}^{i} w_{i}^{j}\\), assuming all \\(w_{i}^{j}\\) are independent with zero mean and variance \\(\\text{Var}[w_{i}]\\), and similarly for \\(w_{j}^{i}\\). Then: \\[\n\\large \\text{Var}[s_{i}]=\\sum_{j=1}^{n_{i}}\\text{Var}[z_{i}^{j}]¬∑\\text{Var}[w_{i}^{j}]=n_{i}¬∑\\text{Var}[z_{i}]\\text{Var}[w_{i}]\n\\] Near the origin, \\(\\large \\text{Var}[z_{i+1}]\\approx \\text{Var}[S_{i}]\\), so we want: \\[\n\\large \\mathrm{Var}[z_{i+1}] \\approx \\mathrm{Var}[z_i] \\Rightarrow n_i \\cdot \\mathrm{Var}[z_i] \\cdot \\mathrm{Var}[w_i] \\approx \\mathrm{Var}[z_i] \\Rightarrow n_i \\cdot \\mathrm{Var}[w_i] \\approx 1\n\\]\n\n\nBackward Pass\nTo compute the gradient \\(\\large \\delta_{i}=\\frac{\\partial L}{\\partial s_{i}}\\), we use the chain rule and the linear approximation \\(\\large \\delta_{i} \\approx W_{i+1}^{T}\\delta_{i+1}\\). Therefore: \\[\n\\mathrm{Var}[\\delta_i] \\approx n_{i+1} \\cdot \\mathrm{Var}[W_i] \\cdot \\mathrm{Var}[\\delta_{i+1}] \\Rightarrow n_{i+1} \\cdot \\mathrm{Var}[W_i] \\approx 1\n\\]\n\n\nA balanced solution\nWe now have two desired conditions:\n\n\\(n_i \\cdot \\mathrm{Var}[w_i] \\approx 1\\)\n\\(n_{i+1} \\cdot \\mathrm{Var}[W_i] \\approx 1\\)\n\nIf \\(n_{i}=n_{i+1}\\), both are satisfied. When not, Glorot and Bengio propose a compromise: \\[\n\\large \\text{Var}[W_{i}]=\\frac{2}{n_{i}+n_{i+1}}\n\\] To implement this in practice, with a uniform distribution \\(W_{i}\\sim \\mathcal{U}[-a,a]\\), note that: \\[\n\\large \\frac{2}{n_i + n_{i+1}} = \\frac{a^2}{3} \\Rightarrow a = \\sqrt{\\frac{6}{n_i + n_{i+1}}}\n\\] This yields the final Xavier Initialization: \\[\n\\Large W_{i}=\\mathcal{U}[-\\sqrt{\\frac{6}{n_{i}+n_{i+1}}},+\\sqrt{\\frac{6}{n_{i}+n_{i+1}}}]\n\\]\nInitializing the parameters according to the equation above will likely solve the vanishing gradient problem. However, all this math is done assuming you have a sigmoid activation function. What about ReLU? That deserves an article for its own.\n\n\n\nReferences\n\nXavier Glorot, Yoshua Bengio, Understanding the Difficulty of Training Deep Feedforward Neural Networks, 2010."
  }
]