
--- 
title: "Why Neural Networks Forget:"
description: "The Vanishing Gradient Problem Explained"
format: html
categories: [deep-learning, ðŸŸ¡medium]
---

# Why Neural Networks Forget: The Vanishing Gradient Problem Explained 

Neural networks adjust their parameters through the well-known backpropagation algorithm. The cost function gradients are propagated from the output layer back to all layers of the model, enabling parameter updates. The networkâ€™s training depends on gradients being properly propagated. If the gradients fail to reach the earlier layers, the network wonâ€™t train as expected. Can something like this actually happen? Letâ€™s see a practical example. 

Consider a layer with a sigmoid activation function. From basic calculus, we know that 
$\large \frac{d\sigma(x)}{dx}=\sigma(x)(1-\sigma(x))$ reaches its maximum value of 0.25 at $x=0$, and is zero when $\sigma(x)$ saturates at 0 or 1. To update the weights of this layer, we need to compute $\large \frac{\partial L}{\partial W^{(L)}}$, which by the chain rule is: 
$$ 
\large \frac{\partial L}{\partial W^{(L)}}=\frac{\partial z^{(L)}}{\partial W^{(L)}}\frac{\partial a^{(L)}}{\partial z^{(L)}}\frac{\partial L}{\partial a^{(L)}}
$$
Focusing on $\large \frac{\partial a^{(L)}}{\partial z^{(L)}}$: 
$$ 
\large \frac{\partial a^{(L)}}{\partial z^{(L)}}=\sigma(z^{(L)})(1-\sigma(z^{L}))
$$
If $\sigma(z^{(L)})=1$ or $\sigma(z^{(L)})=0$, then the gradient is zero. This phenomenon is called **gradient vanishing**. If thereâ€™s no gradient, thereâ€™s no learning. Since the derivative of the sigmoid is always in the range $[0,0.25]$, it's prone to vanishing gradients. 

# The risks of random initialization 

One commonly overlooked detail when training a neural network is weight initialization. If the initial weights push the activation values into saturation zones (0 or 1), gradient vanishing can occur. For example, if weights are initialized as $W\sim\mathcal{N}(0,1)$, some values might lie far outside the optimal region. A natural fix might be to initialize weights as $W\sim\mathcal{U}[-1,1]$, keeping values within range. This may work for small, shallow networks. Let $n$ be the number of neurons in layer $l-1$, meaning the next layer has $n$ inputs. The pre-activation of a neuron is $\sum_{i=1}^n w_{i}x_{i}$, If $n$ is large, the sum may become too large and lead to saturation. So, uniform initialization isnâ€™t ideal either. 

# The Xavier Initialization solution 

If initializing with zeros, Gaussian, or uniform distributions is problematic, whatâ€™s the solution? Fortunately, Xavier Glorot and Yoshua Bengio proposed one in 2010. In their paper _Understanding the Difficulty of Training Deep Feedforward Neural Networks_, they introduced what we now call **Xavier Initialization**. 

### Notation 

Let $s_{i}=z_{i}W_{i}+b_{i}$ be the pre-activation, so $z_{i+1}=f(s_{i})$. The analysis focuses near the origin, where the sigmoid behaves approximately linearly $\sigma(x)\approx x$ and $\sigma'(x)\approx 1$. The goal is to keep the variance of $z_{i+1}$ (forward pass) and the variance of gradients (backward pass) of the same order in every layer.

### Forward Pass 

We write $s_{i}=\sum_{j=1}^n z_{j}^{i} w_{i}^{j}$, assuming all $w_{i}^{j}$ are independent with zero mean and variance $\text{Var}[w_{i}]$, and similarly for $w_{j}^{i}$. Then: 
$$
\large \text{Var}[s_{i}]=\sum_{j=1}^{n_{i}}\text{Var}[z_{i}^{j}]Â·\text{Var}[w_{i}^{j}]=n_{i}Â·\text{Var}[z_{i}]\text{Var}[w_{i}]
$$
Near the origin, $\large \text{Var}[z_{i+1}]\approx \text{Var}[S_{i}]$, so we want: 
$$
\large \mathrm{Var}[z_{i+1}] \approx \mathrm{Var}[z_i] \Rightarrow n_i \cdot \mathrm{Var}[z_i] \cdot \mathrm{Var}[w_i] \approx \mathrm{Var}[z_i] \Rightarrow n_i \cdot \mathrm{Var}[w_i] \approx 1
$$

### Backward Pass 

To compute the gradient $\large \delta_{i}=\frac{\partial L}{\partial s_{i}}$, we use the chain rule and the linear approximation $\large \delta_{i} \approx W_{i+1}^{T}\delta_{i+1}$. Therefore: 
$$
\mathrm{Var}[\delta_i] \approx n_{i+1} \cdot \mathrm{Var}[W_i] \cdot \mathrm{Var}[\delta_{i+1}] \Rightarrow n_{i+1} \cdot \mathrm{Var}[W_i] \approx 1
$$

### A balanced solution 

We now hace two desired conditions: 
- $n_i \cdot \mathrm{Var}[w_i] \approx 1$ 
- $n_{i+1} \cdot \mathrm{Var}[W_i] \approx 1$ 
If $n_{i}=n_{i+1}$, both are satisfied. When not, Glorot and Bengio propose a compromise: 
$$
\large \text{Var}[W_{i}]=\frac{2}{n_{i}+n_{i+1}}
$$
To implement this in practice, with a uniform distribution $W_{i}\sim \mathcal{U}[-a,a]$, note that: 
$$
\large \frac{2}{n_i + n_{i+1}} = \frac{a^2}{3} \Rightarrow a = \sqrt{\frac{6}{n_i + n_{i+1}}}
$$
This yields the final Xavier Initialization: 
$$
\Large W_{i}=\mathcal{U}[-\sqrt{\frac{6}{n_{i}+n_{i+1}}},+\sqrt{\frac{6}{n_{i}+n_{i+1}}}] 
$$

---

### References 

- Xavier Glorot, Yoshua Bengio, _Understanding the Difficulty of Training Deep Feedforward Neural Networks_, 2010.
